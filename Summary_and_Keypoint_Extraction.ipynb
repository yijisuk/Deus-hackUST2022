{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kl5ygjYWrm3",
        "outputId": "dfbd60c6-6157-4439-a641-81de159f6791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.18.1.tar.gz (42 kB)\n",
            "\u001b[?25l\r\u001b[K     |███████▊                        | 10 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 20 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 30 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 40 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 42 kB 591 kB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.2.0.58-py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 34.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.0)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.9)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.18.1-py3-none-any.whl size=53168 sha256=956d7ae78c7e5ce94daed57b525175f03632193f263c2d30a311c4ac69ee954d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/bf/24/fcdc9d2b81f9c7e565bb2036ec9f7cc930056b829895b3bf48\n",
            "Successfully built openai\n",
            "Installing collected packages: pandas-stubs, openai\n",
            "Successfully installed openai-0.18.1 pandas-stubs-1.2.0.58\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.organization = \"\"\n",
        "openai.api_key = \"\""
      ],
      "metadata": {
        "id": "2EYyaulNWt4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original Text:\n",
        "<br>The groundwork concepts for GPT models\n",
        "\n",
        "All these concepts relate to GPT models in some sense. For now, I’ll tell you the definitions (avoiding too much technical detail, although some previous knowledge might be required to follow through). I’ll show later how they are linked to each other and GPT-3.\n",
        "\n",
        "Transformers: This type of neural network appeared in 2017 as a new framework to solve various machine translation problems (these problems are characterized because input and output are sequences). The authors wanted to get rid of convolutions and recurrence (CNNs and RNNs) to rely completely on attention mechanisms. Transformers are state-of-the-art in NLP.\n",
        "\n",
        "Language models: Jason Brownlee defines language models as “probabilistic models that are able to predict the next word in the sequence given the words that precede it.” These models can solve many NLP tasks, such as machine translation, question answering, text summarization, or image captioning.\n",
        "\n",
        "Generative models: In statistics, there are discriminative and generative models, which are often used to perform classification tasks. Discriminative models encode the conditional probability of a given pair of observable and target variables: p(y|x). Generative models encode the joint probability: p(x,y). Generative models can “generate new data similar to existing data,” which is the key idea to take away. Apart from GPT, other popular examples of generative models are GANs (generative adversarial networks) and VAEs (variational autoencoders).\n",
        "\n",
        "Semi-supervised learning: This training paradigm combines unsupervised pre-training with supervised fine-tuning. The idea is to train a model with a very large dataset in an unsupervised way, to then adapt (fine-tune) the model to different tasks, by using supervised training in smaller datasets. This paradigm solves two problems: It doesn’t need many expensive labeled data and tasks without large datasets can be tackled. It’s worth mentioning that GPT-2 and GPT-3 are fully unsupervised (more about this soon).\n",
        "\n",
        "Zero/one/few-shot learning: Usually, deep learning systems are trained and tested for a specific set of classes. If a computer vision system is trained to classify cat, dog, and horse images, it could be tested only on those three classes. In contrast, in zero-shot learning set up the system is shown at test time — without weight updating — classes it has not seen at training time (for instance, testing the system on elephant images). Same thing for one-shot and few-shot settings, but in these cases, at test time the system sees one or few examples of the new classes, respectively. The idea is that a powerful enough system could perform well in these situations, which OpenAI proved with GPT-2 and GPT-3.\n",
        "\n",
        "Multitask learning: Most deep learning systems are single-task. One popular example is AlphaZero. It can learn a few games like chess or Go, but it can only play one type of game at a time. If it knows how to play chess, it doesn’t know how to play Go. Multitask systems overcome this limitation. They’re trained to be able to solve different tasks for a given input. For instance, if I feed the word ‘cat’ to the system, I could ask it to find the Spanish translation ‘gato’, I could ask it to show me the image of a cat, or I could ask it to describe its features. Different tasks for the same input.\n",
        "\n",
        "Zero/one/few-shot task transfer: The idea is to combine the concepts of zero/one/few-shot learning and multitask learning. Instead of showing the system new classes at test time, we could ask it to perform new tasks (either showing it zero, one, or few examples of the new task). For instance, let’s take a system trained in a huge text corpus. In a one-shot task transfer setting we could write: “I love you -> Te quiero. I hate you -> ____.” We are implicitly asking the system to translate a sentence from English to Spanish (a task it hasn’t been trained on) by showing it a single example (one-shot).\n",
        "\n",
        "All these concepts come together in the definition of a GPT model. GPT stands for Generative Pre-Trained. Models of the GPT family have in common that they are language models based in the transformer architecture, pre-trained in a generative, unsupervised manner that show decent performance in zero/one/few-shot multitask settings. This isn’t an explanation of how all these concepts work together in practice, but a simple way to remember that they together build up what a GPT model is. (For deeper explanations I suggest following the links I put above, but only after you’ve read this article!).\n",
        "\n",
        "Reference: https://towardsdatascience.com/gpt-3-a-complete-overview-190232eb25fd"
      ],
      "metadata": {
        "id": "IF0RUkYc2f_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown The models' training data cuts off in October 2019; there may be incapabilities in answering regarding recent context knowledge. Keep that in mind.\n",
        "purpose = \"Key points extraction\" #@param[\"Summary\", \"Key points extraction\"] {type: \"string\"}\n",
        "prompt = \"The groundwork concepts for GPT models  All these concepts relate to GPT models in some sense. For now, I\\u2019ll tell you the definitions (avoiding too much technical detail, although some previous knowledge might be required to follow through). I\\u2019ll show later how they are linked to each other and GPT-3.  Transformers: This type of neural network appeared in 2017 as a new framework to solve various machine translation problems (these problems are characterized because input and output are sequences). The authors wanted to get rid of convolutions and recurrence (CNNs and RNNs) to rely completely on attention mechanisms. Transformers are state-of-the-art in NLP.  Language models: Jason Brownlee defines language models as \\u201Cprobabilistic models that are able to predict the next word in the sequence given the words that precede it.\\u201D These models can solve many NLP tasks, such as machine translation, question answering, text summarization, or image captioning.  Generative models: In statistics, there are discriminative and generative models, which are often used to perform classification tasks. Discriminative models encode the conditional probability of a given pair of observable and target variables: p(y|x). Generative models encode the joint probability: p(x,y). Generative models can \\u201Cgenerate new data similar to existing data,\\u201D which is the key idea to take away. Apart from GPT, other popular examples of generative models are GANs (generative adversarial networks) and VAEs (variational autoencoders).  Semi-supervised learning: This training paradigm combines unsupervised pre-training with supervised fine-tuning. The idea is to train a model with a very large dataset in an unsupervised way, to then adapt (fine-tune) the model to different tasks, by using supervised training in smaller datasets. This paradigm solves two problems: It doesn\\u2019t need many expensive labeled data and tasks without large datasets can be tackled. It\\u2019s worth mentioning that GPT-2 and GPT-3 are fully unsupervised (more about this soon).  Zero/one/few-shot learning: Usually, deep learning systems are trained and tested for a specific set of classes. If a computer vision system is trained to classify cat, dog, and horse images, it could be tested only on those three classes. In contrast, in zero-shot learning set up the system is shown at test time \\u2014 without weight updating \\u2014 classes it has not seen at training time (for instance, testing the system on elephant images). Same thing for one-shot and few-shot settings, but in these cases, at test time the system sees one or few examples of the new classes, respectively. The idea is that a powerful enough system could perform well in these situations, which OpenAI proved with GPT-2 and GPT-3.  Multitask learning: Most deep learning systems are single-task. One popular example is AlphaZero. It can learn a few games like chess or Go, but it can only play one type of game at a time. If it knows how to play chess, it doesn\\u2019t know how to play Go. Multitask systems overcome this limitation. They\\u2019re trained to be able to solve different tasks for a given input. For instance, if I feed the word \\u2018cat\\u2019 to the system, I could ask it to find the Spanish translation \\u2018gato\\u2019, I could ask it to show me the image of a cat, or I could ask it to describe its features. Different tasks for the same input.  Zero/one/few-shot task transfer: The idea is to combine the concepts of zero/one/few-shot learning and multitask learning. Instead of showing the system new classes at test time, we could ask it to perform new tasks (either showing it zero, one, or few examples of the new task). For instance, let\\u2019s take a system trained in a huge text corpus. In a one-shot task transfer setting we could write: \\u201CI love you -> Te quiero. I hate you -> ____.\\u201D We are implicitly asking the system to translate a sentence from English to Spanish (a task it hasn\\u2019t been trained on) by showing it a single example (one-shot).  All these concepts come together in the definition of a GPT model. GPT stands for Generative Pre-Trained. Models of the GPT family have in common that they are language models based in the transformer architecture, pre-trained in a generative, unsupervised manner that show decent performance in zero/one/few-shot multitask settings. This isn\\u2019t an explanation of how all these concepts work together in practice, but a simple way to remember that they together build up what a GPT model is. (For deeper explanations I suggest following the links I put above, but only after you\\u2019ve read this article!).\" #@param {type: \"string\"}\n",
        "#@markdown You may leave the ```answer_requirements``` empty if you don't have one.\n",
        "answer_requirement = \"\" #@param {type: \"string\"}\n",
        "\n",
        "if purpose == \"Summary\":\n",
        "  response = openai.Completion.create(\n",
        "    engine=\"text-davinci-002\",\n",
        "    prompt=f\"{prompt}\\nTl;dr\\n{answer_requirement}\\nA:\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=2000,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0.1,\n",
        "    presence_penalty=0.0,\n",
        "    # stop=[\"\\n\"]\n",
        "  )\n",
        "\n",
        "elif purpose == \"Key points extraction\":\n",
        "  response = openai.Completion.create(\n",
        "    engine=\"text-davinci-002\",\n",
        "    prompt=f\"Q:{prompt}\\nTake out the keywords and define them.\\n{answer_requirement}\\nA:\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=2000,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0.1,\n",
        "    presence_penalty=0.0,\n",
        "    # stop=[\"\\n\"]\n",
        "  )"
      ],
      "metadata": {
        "id": "sC3_AlesW1JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response[\"choices\"][0][\"text\"].split(\"\\n\\n\")[1:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSzAD2g24P9R",
        "outputId": "f257ddb9-5a2a-45dc-fd81-7a6d7be2e76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Transformers: A type of neural network that uses attention mechanisms to solve various machine translation problems.',\n",
              " 'Language models: Probabilistic models that predict the next word in a sequence, based on the words that precede it.',\n",
              " 'Generative models: Models that encode the joint probability of two variables, in order to generate new data similar to existing data.',\n",
              " 'Semi-supervised learning: A training paradigm that combines unsupervised pre-training with supervised fine-tuning, in order to solve tasks without needing a large amount of labeled data.',\n",
              " 'Zero/one/few-shot learning: A situation in which a deep learning system is shown at test time classes it has not seen during training (zero-shot), or is only given one or few examples of the new class (one/few-shot), respectively.',\n",
              " 'Multitask learning: A deep learning system that is trained to solve different tasks for a given input.',\n",
              " 'Zero/one/few-shot task transfer: The combination of zero/one/few-shot learning and multitask learning, in which a system is asked to perform new tasks at test time that it has not been trained on.']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}